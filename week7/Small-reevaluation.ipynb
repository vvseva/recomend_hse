{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Примеры составлены по мотивам главы 2 книги Т.Сегаран \"Программируем коллективный разум\" http://www.symbol.ru/alphabet/613828.html\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Словарь с предпочтениями пользователи-фильмы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Словарь кинокритиков и выставленных ими оценок для небольшого набора\n",
    "# данных о фильмах\n",
    "critics={'Lisa Rose': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,\n",
    " 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \n",
    " 'The Night Listener': 3.0},\n",
    "'Gene Seymour': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \n",
    " 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \n",
    " 'You, Me and Dupree': 3.5}, \n",
    "'Michael Phillips': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,\n",
    " 'Superman Returns': 3.5, 'The Night Listener': 4.0},\n",
    "'Claudia Puig': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,\n",
    " 'The Night Listener': 4.5, 'Superman Returns': 4.0, \n",
    " 'You, Me and Dupree': 2.5},\n",
    "'Mick LaSalle': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \n",
    " 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,\n",
    " 'You, Me and Dupree': 2.0}, \n",
    "'Jack Matthews': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n",
    " 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},\n",
    "'Toby': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0,'Superman Returns':4.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.5, 3.5, 3. , 3.5, 2.5, 3. ],\n",
       "       [3. , 3.5, 1.5, 5. , 3.5, 3. ],\n",
       "       [2.5, 3. , nan, 3.5, nan, 4. ],\n",
       "       [nan, 3.5, 3. , 4. , 2.5, 4.5],\n",
       "       [3. , 4. , 2. , 3. , 2. , 3. ],\n",
       "       [3. , 4. , nan, 5. , 3.5, 3. ],\n",
       "       [nan, 4.5, nan, 4. , 1. , nan]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_base = pd.DataFrame(critics).as_matrix().T\n",
    "rates_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5, 3.5, 3. , 3.5, 2.5, 3. ],\n",
       "       [3. , 3.5, 1.5, 5. , 3.5, 3. ],\n",
       "       [2.5, 3. , 0. , 3.5, 0. , 4. ],\n",
       "       [0. , 3.5, 3. , 4. , 2.5, 4.5],\n",
       "       [3. , 4. , 2. , 3. , 2. , 3. ],\n",
       "       [3. , 4. , 0. , 5. , 3.5, 3. ],\n",
       "       [0. , 4.5, 0. , 4. , 1. , 0. ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates = np.nan_to_num(rates_base)\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import solve\n",
    "\n",
    "\n",
    "class ExplicitMF():\n",
    "    def __init__(self, \n",
    "                 ratings,\n",
    "                 n_factors=40,\n",
    "                 learning='sgd',\n",
    "                 item_fact_reg=0.0, \n",
    "                 user_fact_reg=0.0,\n",
    "                 item_bias_reg=0.0,\n",
    "                 user_bias_reg=0.0,\n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict empty \n",
    "        entries in a matrix. The terminology assumes a \n",
    "        ratings matrix which is ~ user x item\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        ratings : (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        n_factors : (int)\n",
    "            Number of latent factors to use in matrix \n",
    "            factorization model\n",
    "        learning : (str)\n",
    "            Method of optimization. Options include \n",
    "            'sgd' or 'als'.\n",
    "        \n",
    "        item_fact_reg : (float)\n",
    "            Regularization term for item latent factors\n",
    "        \n",
    "        user_fact_reg : (float)\n",
    "            Regularization term for user latent factors\n",
    "            \n",
    "        item_bias_reg : (float)\n",
    "            Regularization term for item biases\n",
    "        \n",
    "        user_bias_reg : (float)\n",
    "            Regularization term for user biases\n",
    "        \n",
    "        verbose : (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.n_users, self.n_items = ratings.shape\n",
    "        self.n_factors = n_factors\n",
    "        self.item_fact_reg = item_fact_reg\n",
    "        self.user_fact_reg = user_fact_reg\n",
    "        self.item_bias_reg = item_bias_reg\n",
    "        self.user_bias_reg = user_bias_reg\n",
    "        self.learning = learning\n",
    "        if self.learning == 'sgd':\n",
    "            self.sample_row, self.sample_col = self.ratings.nonzero()\n",
    "            self.n_samples = len(self.sample_row)\n",
    "        self._v = verbose\n",
    "\n",
    "    def als_step(self,\n",
    "                 latent_vectors,\n",
    "                 fixed_vecs,\n",
    "                 ratings,\n",
    "                 _lambda,\n",
    "                 type='user'):\n",
    "        \"\"\"\n",
    "        One of the two ALS steps. Solve for the latent vectors\n",
    "        specified by type.\n",
    "        \"\"\"\n",
    "        if type == 'user':\n",
    "            # Precompute\n",
    "            YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(YTY.shape[0]) * _lambda\n",
    "\n",
    "            for u in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[u, :] = solve((YTY + lambdaI), \n",
    "                                             ratings[u, :].dot(fixed_vecs))\n",
    "        elif type == 'item':\n",
    "            # Precompute\n",
    "            XTX = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(XTX.shape[0]) * _lambda\n",
    "            \n",
    "            for i in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[i, :] = solve((XTX + lambdaI), \n",
    "                                             ratings[:, i].T.dot(fixed_vecs))\n",
    "        return latent_vectors\n",
    "\n",
    "    def train(self, n_iter=10, learning_rate=0.1):\n",
    "        \"\"\" Train model for n_iter iterations from scratch.\"\"\"\n",
    "        # initialize latent vectors        \n",
    "        self.user_vecs = np.random.normal(scale=1./self.n_factors, size=(self.n_users, self.n_factors))\n",
    "        self.item_vecs = np.random.normal(scale=1./self.n_factors, size=(self.n_items, self.n_factors))\n",
    "        \n",
    "        if self.learning == 'als':\n",
    "            self.partial_train(n_iter,0)\n",
    "        elif self.learning == 'sgd':\n",
    "            self.learning_rate = learning_rate\n",
    "            self.user_bias = np.zeros(self.n_users)\n",
    "            self.item_bias = np.zeros(self.n_items)\n",
    "            self.global_bias = np.mean(self.ratings[np.where(self.ratings != 0)])\n",
    "            self.partial_train(n_iter,0)\n",
    "    \n",
    "    \n",
    "    def partial_train(self, n_iter, iter_done):\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. Can be \n",
    "        called multiple times for further training.\n",
    "        \"\"\"\n",
    "        ctr = 1\n",
    "        while ctr <= n_iter:\n",
    "            if (ctr+iter_done) % 10 == 0 and self._v:\n",
    "                print (f'\\tcurrent iteration: {ctr+iter_done}')\n",
    "            if self.learning == 'als':\n",
    "                self.user_vecs = self.als_step(self.user_vecs, \n",
    "                                               self.item_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.user_fact_reg, \n",
    "                                               type='user')\n",
    "                self.item_vecs = self.als_step(self.item_vecs, \n",
    "                                               self.user_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.item_fact_reg, \n",
    "                                               type='item')\n",
    "            elif self.learning == 'sgd':\n",
    "                self.training_indices = np.arange(self.n_samples)\n",
    "                np.random.shuffle(self.training_indices)\n",
    "                self.sgd()\n",
    "            ctr += 1\n",
    "\n",
    "    def sgd(self):\n",
    "        for idx in self.training_indices:\n",
    "            u = self.sample_row[idx]\n",
    "            i = self.sample_col[idx]\n",
    "            prediction = self.predict(u, i)\n",
    "            e = (self.ratings[u,i] - prediction) # error\n",
    "            \n",
    "            # Update biases\n",
    "            self.user_bias[u] += self.learning_rate * (e - self.user_bias_reg * self.user_bias[u])\n",
    "            self.item_bias[i] += self.learning_rate * (e - self.item_bias_reg * self.item_bias[i])\n",
    "            \n",
    "            #Update latent factors\n",
    "            \n",
    "            delta_i = self.learning_rate * (e * self.user_vecs[u, :] - self.item_fact_reg * self.item_vecs[i,:])\n",
    "            \n",
    "            self.user_vecs[u, :] += self.learning_rate *\\\n",
    "                                    (e * self.item_vecs[i, :] - self.user_fact_reg * self.user_vecs[u,:])\n",
    "#            self.item_vecs[i, :] += self.learning_rate * \\\n",
    "#                                    (e * self.user_vecs[u, :] - self.item_fact_reg * self.item_vecs[i,:])\n",
    "            self.item_vecs[i, :] += delta_i\n",
    "   \n",
    "\n",
    "    def predict(self, u, i):\n",
    "        \"\"\" Single user and item prediction.\"\"\"\n",
    "        if self.learning == 'als':\n",
    "            return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "        elif self.learning == 'sgd':\n",
    "            prediction = self.global_bias + self.user_bias[u] + self.item_bias[i]\n",
    "            prediction += self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "            return prediction\n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\" Predict ratings for every user and item.\"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], \n",
    "                                self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "                \n",
    "        return predictions\n",
    "    \n",
    "    def calculate_learning_curve(self, iter_array, test, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Keep track of MSE as a function of training iterations.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        iter_array : (list)\n",
    "            List of numbers of iterations to train for each step of \n",
    "            the learning curve. e.g. [1, 5, 10, 20]\n",
    "        test : (2D ndarray)\n",
    "            Testing dataset (assumed to be user x item).\n",
    "        \n",
    "        The function creates two new class attributes:\n",
    "        \n",
    "        train_mse : (list)\n",
    "            Training data MSE values for each value of iter_array\n",
    "        test_mse : (list)\n",
    "            Test data MSE values for each value of iter_array\n",
    "        \"\"\"\n",
    "        iter_array.sort()\n",
    "        self.train_mse =[]\n",
    "        self.test_mse = []\n",
    "        iter_diff = 0\n",
    "        for (i, n_iter) in enumerate(iter_array):\n",
    "            if self._v:\n",
    "                print (f'Iteration: {n_iter}')\n",
    "            if i == 0:\n",
    "                self.train(n_iter - iter_diff, learning_rate)\n",
    "            else:\n",
    "                self.partial_train(n_iter - iter_diff, iter_diff)\n",
    "\n",
    "            predictions = self.predict_all()\n",
    "\n",
    "            self.train_mse += [get_mse(predictions, self.ratings)]\n",
    "            self.test_mse += [get_mse(predictions, test)]\n",
    "            if self._v:\n",
    "                print (f'MSE train:test: {round(self.train_mse[-1],2)} : {round(self.test_mse[-1],2)}\\n')\n",
    "            iter_diff = n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test = ExplicitMF(rates, n_factors = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "small_test.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40217248,  0.25797608],\n",
       "       [-0.16243287,  0.87598517],\n",
       "       [ 0.16100939, -0.25233535],\n",
       "       [ 0.08975379,  0.07907003],\n",
       "       [ 0.86469985,  0.67316213],\n",
       "       [ 0.01513168,  0.93212874],\n",
       "       [ 0.47573728, -1.66879674]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.user_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15213102,  0.45500027],\n",
       "       [ 0.64522574, -0.46699448],\n",
       "       [ 0.522016  ,  0.69195765],\n",
       "       [ 0.28731734,  0.59807869],\n",
       "       [-0.41869311,  0.3449829 ],\n",
       "       [ 0.45565799, -0.45419218],\n",
       "       [-1.03686891, -0.9714227 ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.train(100)\n",
    "small_test.user_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40217248,  0.25797608],\n",
       "       [-0.16243287,  0.87598517],\n",
       "       [ 0.16100939, -0.25233535],\n",
       "       [ 0.08975379,  0.07907003],\n",
       "       [ 0.86469985,  0.67316213],\n",
       "       [ 0.01513168,  0.93212874],\n",
       "       [ 0.47573728, -1.66879674]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "small_test.train(100)\n",
    "small_test.user_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43723467,  0.37413166],\n",
       "       [-0.20754576,  0.61306766],\n",
       "       [ 0.17121251, -0.02789465],\n",
       "       [ 0.02890269,  0.114141  ],\n",
       "       [ 0.80175832,  0.51259266],\n",
       "       [-0.04298589,  0.6792025 ],\n",
       "       [ 0.5180563 , -2.14187129]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.partial_train(100, 100)\n",
    "small_test.user_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:145: RuntimeWarning: overflow encountered in multiply\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:145: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:143: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:139: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan, nan],\n",
       "       [nan, nan],\n",
       "       [nan, nan],\n",
       "       [nan, nan],\n",
       "       [nan, nan],\n",
       "       [nan, nan],\n",
       "       [nan, nan]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.partial_train(100, 200)\n",
    "small_test.user_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27924925,  0.37524454],\n",
       "       [-0.26663675, -0.84067193],\n",
       "       [ 0.81720107, -0.02148859],\n",
       "       [ 0.7002485 , -0.19122691],\n",
       "       [-0.10791849,  0.97323666],\n",
       "       [-0.34557659, -0.52839578],\n",
       "       [ 3.78626345,  0.88902859]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38012637  0.19552859 -0.50322135 -0.00748267 -0.59051758  0.2819706\n",
      "  1.23284412]\n",
      "[-0.22808433  0.60370324 -0.91464046  0.95547614 -0.32068337  0.13322413]\n"
     ]
    }
   ],
   "source": [
    "print(small_test.item_bias)\n",
    "print(small_test.user_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30649008,  0.12739635],\n",
       "       [ 0.19952093,  1.4053112 ],\n",
       "       [ 0.31643079, -0.41136693],\n",
       "       [ 0.42886522, -0.16968078],\n",
       "       [ 0.58307119,  0.33139668],\n",
       "       [-0.05483718,  0.85933672],\n",
       "       [ 0.47526805,  0.28593246]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "small_test.train(100, 0.01)\n",
    "small_test.user_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4735413 ,  0.48497761],\n",
       "       [-0.35966023,  0.51521259],\n",
       "       [ 0.26919599,  0.15274652],\n",
       "       [ 0.16228673,  0.16575106],\n",
       "       [ 0.87036004,  0.81335548],\n",
       "       [-0.17200936,  0.62457611],\n",
       "       [ 1.69899774, -3.44284765]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.partial_train(10000, 100)\n",
    "small_test.user_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.71181919,  3.56602721,  2.54366178,  3.48060826,  2.26261535,\n",
       "         3.4055833 ],\n",
       "       [ 2.88231654,  3.57973425,  1.51633611,  5.01708247,  3.50785519,\n",
       "         2.99430229],\n",
       "       [ 2.4140297 ,  3.06437845,  3.02911036,  3.5117287 ,  2.03031617,\n",
       "         4.00950421],\n",
       "       [ 2.78176916,  3.41644378,  3.21422022,  4.05490247,  2.54127941,\n",
       "         4.27453587],\n",
       "       [ 2.88243438,  3.97824584,  2.19781266,  3.00682275,  2.12129594,\n",
       "         2.79925332],\n",
       "       [ 3.10904168,  3.89834679,  1.64445015,  4.93825338,  3.56028656,\n",
       "         2.99482832],\n",
       "       [ 5.30342166,  4.49705987, 19.21100138,  3.99341183,  1.00859768,\n",
       "        18.79395698]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.predict_all() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.35744271,  0.43000948, -0.47329869, -0.02721962, -0.51156278,\n",
       "        0.50731166,  1.67688811])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.user_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.55522546, -0.03467747,  0.18818513,  0.98088308, -0.68648891,\n",
       "        1.35200906])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.item_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2285714285714286"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.global_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.257468596506937"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.global_bias + small_test.item_bias[-1] +small_test.user_bias[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.69899774, -3.44284765])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.user_vecs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.33039651, -2.98478143])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.item_vecs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21181919,  0.06602721, -0.45633822, -0.01939174, -0.23738465,\n",
       "         0.4055833 ],\n",
       "       [-0.11768346,  0.07973425,  0.01633611,  0.01708247,  0.00785519,\n",
       "        -0.00569771],\n",
       "       [-0.0859703 ,  0.06437845,         nan,  0.0117287 ,         nan,\n",
       "         0.00950421],\n",
       "       [        nan, -0.08355622,  0.21422022,  0.05490247,  0.04127941,\n",
       "        -0.22546413],\n",
       "       [-0.11756562, -0.02175416,  0.19781266,  0.00682275,  0.12129594,\n",
       "        -0.20074668],\n",
       "       [ 0.10904168, -0.10165321,         nan, -0.06174662,  0.06028656,\n",
       "        -0.00517168],\n",
       "       [        nan, -0.00294013,         nan, -0.00658817,  0.00859768,\n",
       "                nan]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.predict_all() - rates_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30829193,  0.12771461],\n",
       "       [ 0.19911752,  1.40894392],\n",
       "       [ 0.32238188, -0.4113416 ],\n",
       "       [ 0.43407689, -0.1698714 ],\n",
       "       [ 0.58113064,  0.32870178],\n",
       "       [-0.05819788,  0.86094874],\n",
       "       [ 0.47873408,  0.28072167]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test = ExplicitMF(rates, n_factors = 2)\n",
    "np.random.seed(0)\n",
    "small_test.train(100, 0.01)\n",
    "small_test.user_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49943859,  0.53823666],\n",
       "       [-0.43743264,  0.70516984],\n",
       "       [ 0.28289139,  0.11430185],\n",
       "       [ 0.15520764,  0.15590224],\n",
       "       [ 0.93993089,  0.9189709 ],\n",
       "       [-0.22474213,  0.82083514],\n",
       "       [ 1.59927371, -3.16478866]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.partial_train(10000, 100)\n",
    "small_test.user_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22260291,  0.07650373, -0.44802229, -0.00627089, -0.25144248,\n",
       "         0.41374775],\n",
       "       [-0.11989419,  0.07138466,  0.02848343,  0.00812727,  0.0238319 ,\n",
       "        -0.01134542],\n",
       "       [-0.08645487,  0.06426477,         nan,  0.01202391,         nan,\n",
       "         0.01022411],\n",
       "       [        nan, -0.06476354,  0.20587848,  0.08277888, -0.00352276,\n",
       "        -0.22518199],\n",
       "       [-0.12203006, -0.02725635,  0.21669661, -0.0025042 ,  0.12690145,\n",
       "        -0.18974205],\n",
       "       [ 0.10645137, -0.11037187,         nan, -0.07812818,  0.07952442,\n",
       "         0.00190345],\n",
       "       [        nan, -0.00792694,         nan, -0.01487834,  0.01866038,\n",
       "                nan]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test.predict_all() - rates_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "lesson11_recsys.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
